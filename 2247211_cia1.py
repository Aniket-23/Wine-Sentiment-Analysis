# -*- coding: utf-8 -*-
"""2247211_CIA1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q_LGz_8l3zWJjIHc7HasJMOaDpfhY-hW

# Initial phase
"""

import pandas as pd
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt')

# Load the dataset
data = pd.read_csv('wine-reviews.csv')

# display the loaded dataset
data

data.shape

# check if there are any null values
data.isna().sum()

# Access the description column
descriptions = data['description']

descriptions

"""# Tokenization"""

# Tokenize the descriptions
tokenized_descriptions = descriptions.apply(word_tokenize)

# Print the tokenized descriptions
print(tokenized_descriptions)

"""# Word Cloud for filtered tokens"""

# !pip install wordcloud

from nltk.corpus import stopwords
from wordcloud import WordCloud
import matplotlib.pyplot as plt

nltk.download('stopwords')

# Combine all tokenized descriptions into a single string
all_descriptions = ' '.join([word for sublist in tokenized_descriptions for word in sublist])

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Stemming"""

from nltk.stem import PorterStemmer

# Initialize the Porter stemmer
stemmer = PorterStemmer()

# Define a function to apply stemming to a sentence
def stem_sentence(sentence):
    tokenized_words = word_tokenize(sentence)  # Tokenize the sentence into words
    stemmed_words = [stemmer.stem(word) for word in tokenized_words]  # Apply stemming to each word
    return ' '.join(stemmed_words)  # Join the stemmed words back into a sentence

# Apply stemming to the description column
stemmed_descriptions = descriptions.apply(stem_sentence)

# Print the tokens and stemmed descriptions
print("TOKENIZED DESCRIPTION:")
print(tokenized_descriptions)
print("\nSTEMMED TOKENS:")
print(stemmed_descriptions)

"""# Word Cloud for Stemmed Description"""

# Combine all stemmed descriptions into a single string
all_descriptions = ' '.join(stemmed_descriptions)

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Lemmatization"""

from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# Define a function to apply lemmatization to a sentence
def lemmatize_sentence(sentence):
    tokenized_words = word_tokenize(sentence)  # Tokenize the sentence into words
    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokenized_words]  # Apply lemmatization to each word
    return ' '.join(lemmatized_words)  # Join the lemmatized words back into a sentence

# Apply lemmatization to the description column
lemmatized_descriptions = descriptions.apply(lemmatize_sentence)

# Print the tokenized description and lemmatized descriptions
print("TOKENIZED DESCRIPTION:")
print(tokenized_descriptions)
print("\nLEMMATIZED DESCRIPTION:")
print(lemmatized_descriptions)

"""# Word cloud for Lemmatized Description"""

# Combine all lemmatized descriptions into a single string
all_descriptions = ' '.join(lemmatized_descriptions)

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_descriptions)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Identification of stop words and filtering them"""

from nltk.corpus import stopwords

nltk.download('stopwords')

# Get the list of English stopwords
stop_words = set(stopwords.words('english'))

# Filter out the stop words from the tokenized descriptions
filtered_descriptions = tokenized_descriptions.apply(lambda tokens: [token for token in tokens if token.lower() not in stop_words])

# Print the tokenized description and filtered descriptions
print("TOKENIZED DESCRIPTION:")
print(tokenized_descriptions)
print("\nFILTERED DESCRIPTION:")
print(filtered_descriptions)

"""# Word cloud for Filtered Description"""

# Combine all filtered descriptions into a single list of tokens
filtered_tokens = [token for tokens in filtered_descriptions for token in tokens]

# Combine all filtered tokens into a single string
all_filtered_tokens = ' '.join(filtered_tokens)

# Create a WordCloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_filtered_tokens)

# Plot the word cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# Sentiment analysis on the descriptions"""

nltk.download('vader_lexicon')

from nltk.sentiment import SentimentIntensityAnalyzer

# Initialize the VADER sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Perform sentiment analysis on the descriptions
sentiment_scores = descriptions.apply(lambda description: analyzer.polarity_scores(description))

# Extract the compound sentiment score
compound_scores = sentiment_scores.apply(lambda score: score['compound'])

# Classify the sentiment based on the compound score
sentiment_class = compound_scores.apply(lambda score: 'Positive' if score >= 0 else 'Negative')

# Print the sentiment class
print(sentiment_class)

print(sentiment_scores)

"""# Web application"""

# pip install flask nltk

